---
title: "How I trained ML models across distributed nodes without moving any data"
description: "A federated learning pipeline for 5G anomaly detection using Kafka, gRPC, and LSTM autoencoders."
date: "2025-03-15"
tags: ["Kafka", "TensorFlow", "gRPC", "Docker", "Distributed Systems", "Python"]
---

The usual approach to anomaly detection is to collect data from all your nodes, dump it somewhere central, and train on it there. That works fine until the data is sensitive, the nodes are remote, or the volume makes centralizing it impractical. Federated learning is the alternative: each node trains locally on its own data and only the model weights travel across the network, never the raw data itself. I wanted to build this properly end to end, so I did.

The system has five modules, each running in its own Docker container. `DB_Splitter` takes the dataset and partitions it across nodes before anything starts, simulating the reality of data being distributed. `FLClients` is the code that runs on each node. `FLServer` is the aggregation side. `FLInference` handles real-time scoring once the model is trained. `FLConfig` holds configuration shared across all of them.

<FedSAArchitecture />

On each client, network metrics come in through Kafka. I wrote a `ProducerConsumer` class that uses threads and semaphores to keep the data pipeline running, with freshness tracking so the training process doesn't accidentally learn from stale samples. The model is an LSTM autoencoder. It learns to reconstruct normal traffic patterns, so when something anomalous happens the reconstruction error spikes and you catch it. Training happens entirely locally and the raw data never leaves the node.

After each round of local training, the client serializes its model weights with Protobuf and sends them to the server over gRPC. The server collects weights from all clients and runs FedAvg to produce an updated global model, then pushes it back out. I also implemented FedAvgM which adds momentum to the aggregation, and a semi-async variant that doesn't wait for all nodes to finish before aggregating. That last one noticeably improved convergence time because in any real distributed setup some nodes are just slower, and blocking on the slowest one hurts everyone.

<FedSARoundFlow />

Two things that caused more friction than expected. gRPC has a default max message size of 4MB. LSTM model weights are nowhere near that small, I had to raise the limit to 390MB. The other one was NTP. The freshness tracking logic compares timestamps across nodes to decide whether a sample is recent enough to train on. If the clocks on different machines are even slightly out of sync the logic breaks and you start training on old data without realizing it.
